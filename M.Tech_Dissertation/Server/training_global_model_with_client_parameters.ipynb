{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "919c1e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pathlib\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa21ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- CONFIG --------------------\n",
    "FNN_CLIENT_ROOT  = pathlib.Path(\"/Users/sohinikar/FL/M.Tech_Dissertation/Client/client_params/FNN_BC\")\n",
    "LSTM_CLIENT_ROOT = pathlib.Path(\"/Users/sohinikar/FL/M.Tech_Dissertation/Client/client_params/LSTM_BC\")\n",
    "\n",
    "FNN_GLOBAL_PATH  = pathlib.Path(\"/Users/sohinikar/FL/M.Tech_Dissertation/Server/global_model/global_FNN_model.keras\")\n",
    "LSTM_GLOBAL_PATH = pathlib.Path(\"/Users/sohinikar/FL/M.Tech_Dissertation/Server/global_model/global_lstm_model.keras\")\n",
    "\n",
    "# Weight by client sample counts found in params.json (\"samples\")\n",
    "WEIGHT_BY_SAMPLES = True\n",
    "\n",
    "# Save updated copies alongside the originals (timestamped)\n",
    "TS = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "FNN_OUT_MODEL  = FNN_GLOBAL_PATH.with_name(f\"{FNN_GLOBAL_PATH.stem}_AGG_{TS}.keras\")\n",
    "FNN_OUT_WEIGHTS = FNN_GLOBAL_PATH.with_name(f\"{FNN_GLOBAL_PATH.stem}_AGG_{TS}.weights.h5\")\n",
    "LSTM_OUT_MODEL = LSTM_GLOBAL_PATH.with_name(f\"{LSTM_GLOBAL_PATH.stem}_AGG_{TS}.keras\")\n",
    "LSTM_OUT_WEIGHTS = LSTM_GLOBAL_PATH.with_name(f\"{LSTM_GLOBAL_PATH.stem}_AGG_{TS}.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c19f50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Aggregating FNN global model ===\n",
      "Found 128 client(s) in /Users/sohinikar/FL/M.Tech_Dissertation/Client/client_params/FNN_BC\n",
      "  ✓ user_001: included (weight=46)\n",
      "  ✓ user_002: included (weight=46)\n",
      "  ✓ user_003: included (weight=46)\n",
      "  ✓ user_004: included (weight=46)\n",
      "  ✓ user_005: included (weight=46)\n",
      "  ✓ user_006: included (weight=46)\n",
      "  ✓ user_007: included (weight=46)\n",
      "  ✓ user_008: included (weight=46)\n",
      "  ✓ user_009: included (weight=46)\n",
      "  ✓ user_010: included (weight=46)\n",
      "  ✓ user_011: included (weight=46)\n",
      "  ✓ user_012: included (weight=46)\n",
      "  ✓ user_013: included (weight=46)\n",
      "  ✓ user_014: included (weight=46)\n",
      "  ✓ user_015: included (weight=46)\n",
      "  ✓ user_016: included (weight=46)\n",
      "  ✓ user_017: included (weight=46)\n",
      "  ✓ user_018: included (weight=46)\n",
      "  ✓ user_019: included (weight=46)\n",
      "  ✓ user_020: included (weight=46)\n",
      "  ✓ user_021: included (weight=46)\n",
      "  ✓ user_022: included (weight=46)\n",
      "  ✓ user_023: included (weight=46)\n",
      "  ✓ user_024: included (weight=46)\n",
      "  ✓ user_025: included (weight=46)\n",
      "  ✓ user_026: included (weight=46)\n",
      "  ✓ user_027: included (weight=46)\n",
      "  ✓ user_028: included (weight=46)\n",
      "  ✓ user_029: included (weight=46)\n",
      "  ✓ user_030: included (weight=46)\n",
      "  ✓ user_031: included (weight=46)\n",
      "  ✓ user_032: included (weight=46)\n",
      "  ✓ user_033: included (weight=46)\n",
      "  ✓ user_034: included (weight=46)\n",
      "  ✓ user_035: included (weight=46)\n",
      "  ✓ user_036: included (weight=46)\n",
      "  ✓ user_037: included (weight=46)\n",
      "  ✓ user_038: included (weight=46)\n",
      "  ✓ user_039: included (weight=46)\n",
      "  ✓ user_040: included (weight=46)\n",
      "  ✓ user_041: included (weight=46)\n",
      "  ✓ user_042: included (weight=46)\n",
      "  ✓ user_043: included (weight=46)\n",
      "  ✓ user_044: included (weight=46)\n",
      "  ✓ user_045: included (weight=46)\n",
      "  ✓ user_046: included (weight=46)\n",
      "  ✓ user_047: included (weight=46)\n",
      "  ✓ user_048: included (weight=46)\n",
      "  ✓ user_049: included (weight=46)\n",
      "  ✓ user_050: included (weight=46)\n",
      "  ✓ user_051: included (weight=46)\n",
      "  ✓ user_052: included (weight=46)\n",
      "  ✓ user_053: included (weight=46)\n",
      "  ✓ user_054: included (weight=46)\n",
      "  ✓ user_055: included (weight=46)\n",
      "  ✓ user_056: included (weight=46)\n",
      "  ✓ user_057: included (weight=46)\n",
      "  ✓ user_058: included (weight=46)\n",
      "  ✓ user_059: included (weight=46)\n",
      "  ✓ user_060: included (weight=46)\n",
      "  ✓ user_061: included (weight=46)\n",
      "  ✓ user_062: included (weight=46)\n",
      "  ✓ user_063: included (weight=46)\n",
      "  ✓ user_064: included (weight=46)\n",
      "  ✓ user_065: included (weight=46)\n",
      "  ✓ user_066: included (weight=46)\n",
      "  ✓ user_067: included (weight=46)\n",
      "  ✓ user_068: included (weight=46)\n",
      "  ✓ user_069: included (weight=46)\n",
      "  ✓ user_070: included (weight=46)\n",
      "  ✓ user_071: included (weight=46)\n",
      "  ✓ user_072: included (weight=46)\n",
      "  ✓ user_073: included (weight=46)\n",
      "  ✓ user_074: included (weight=46)\n",
      "  ✓ user_075: included (weight=46)\n",
      "  ✓ user_076: included (weight=46)\n",
      "  ✓ user_077: included (weight=46)\n",
      "  ✓ user_078: included (weight=46)\n",
      "  ✓ user_079: included (weight=46)\n",
      "  ✓ user_080: included (weight=46)\n",
      "  ✓ user_081: included (weight=46)\n",
      "  ✓ user_082: included (weight=46)\n",
      "  ✓ user_083: included (weight=46)\n",
      "  ✓ user_084: included (weight=46)\n",
      "  ✓ user_085: included (weight=46)\n",
      "  ✓ user_086: included (weight=46)\n",
      "  ✓ user_087: included (weight=46)\n",
      "  ✓ user_088: included (weight=46)\n",
      "  ✓ user_089: included (weight=46)\n",
      "  ✓ user_090: included (weight=46)\n",
      "  ✓ user_091: included (weight=46)\n",
      "  ✓ user_092: included (weight=46)\n",
      "  ✓ user_093: included (weight=46)\n",
      "  ✓ user_094: included (weight=46)\n",
      "  ✓ user_095: included (weight=46)\n",
      "  ✓ user_096: included (weight=46)\n",
      "  ✓ user_097: included (weight=46)\n",
      "  ✓ user_098: included (weight=46)\n",
      "  ✓ user_099: included (weight=46)\n",
      "  ✓ user_100: included (weight=46)\n",
      "  ✓ user_101: included (weight=46)\n",
      "  ✓ user_102: included (weight=46)\n",
      "  ✓ user_103: included (weight=46)\n",
      "  ✓ user_104: included (weight=46)\n",
      "  ✓ user_105: included (weight=46)\n",
      "  ✓ user_106: included (weight=46)\n",
      "  ✓ user_107: included (weight=46)\n",
      "  ✓ user_108: included (weight=46)\n",
      "  ✓ user_109: included (weight=46)\n",
      "  ✓ user_110: included (weight=46)\n",
      "  ✓ user_111: included (weight=46)\n",
      "  ✓ user_112: included (weight=46)\n",
      "  ✓ user_113: included (weight=46)\n",
      "  ✓ user_114: included (weight=46)\n",
      "  ✓ user_115: included (weight=46)\n",
      "  ✓ user_116: included (weight=46)\n",
      "  ✓ user_117: included (weight=46)\n",
      "  ✓ user_118: included (weight=46)\n",
      "  ✓ user_119: included (weight=46)\n",
      "  ✓ user_120: included (weight=46)\n",
      "  ✓ user_121: included (weight=46)\n",
      "  ✓ user_122: included (weight=46)\n",
      "  ✓ user_123: included (weight=46)\n",
      "  ✓ user_124: included (weight=46)\n",
      "  ✓ user_125: included (weight=46)\n",
      "  ✓ user_126: included (weight=46)\n",
      "  ✓ user_127: included (weight=46)\n",
      "  ✓ user_128: included (weight=46)\n",
      "\n",
      "✅ Aggregated 128 client(s) (total weight=5888) into:\n",
      "   Model  : /Users/sohinikar/FL/M.Tech_Dissertation/Server/global_model/global_FNN_model_AGG_20250830_015056.keras\n",
      "   Weights: /Users/sohinikar/FL/M.Tech_Dissertation/Server/global_model/global_FNN_model_AGG_20250830_015056.weights.h5\n",
      "\n",
      "=== Aggregating LSTM global model ===\n",
      "Found 128 client(s) in /Users/sohinikar/FL/M.Tech_Dissertation/Client/client_params/LSTM_BC\n",
      "  ✓ user_001: included (weight=46)\n",
      "  ✓ user_002: included (weight=46)\n",
      "  ✓ user_003: included (weight=46)\n",
      "  ✓ user_004: included (weight=46)\n",
      "  ✓ user_005: included (weight=46)\n",
      "  ✓ user_006: included (weight=46)\n",
      "  ✓ user_007: included (weight=46)\n",
      "  ✓ user_008: included (weight=46)\n",
      "  ✓ user_009: included (weight=46)\n",
      "  ✓ user_010: included (weight=46)\n",
      "  ✓ user_011: included (weight=46)\n",
      "  ✓ user_012: included (weight=46)\n",
      "  ✓ user_013: included (weight=46)\n",
      "  ✓ user_014: included (weight=46)\n",
      "  ✓ user_015: included (weight=46)\n",
      "  ✓ user_016: included (weight=46)\n",
      "  ✓ user_017: included (weight=46)\n",
      "  ✓ user_018: included (weight=46)\n",
      "  ✓ user_019: included (weight=46)\n",
      "  ✓ user_020: included (weight=46)\n",
      "  ✓ user_021: included (weight=46)\n",
      "  ✓ user_022: included (weight=46)\n",
      "  ✓ user_023: included (weight=46)\n",
      "  ✓ user_024: included (weight=46)\n",
      "  ✓ user_025: included (weight=46)\n",
      "  ✓ user_026: included (weight=46)\n",
      "  ✓ user_027: included (weight=46)\n",
      "  ✓ user_028: included (weight=46)\n",
      "  ✓ user_029: included (weight=46)\n",
      "  ✓ user_030: included (weight=46)\n",
      "  ✓ user_031: included (weight=46)\n",
      "  ✓ user_032: included (weight=46)\n",
      "  ✓ user_033: included (weight=46)\n",
      "  ✓ user_034: included (weight=46)\n",
      "  ✓ user_035: included (weight=46)\n",
      "  ✓ user_036: included (weight=46)\n",
      "  ✓ user_037: included (weight=46)\n",
      "  ✓ user_038: included (weight=46)\n",
      "  ✓ user_039: included (weight=46)\n",
      "  ✓ user_040: included (weight=46)\n",
      "  ✓ user_041: included (weight=46)\n",
      "  ✓ user_042: included (weight=46)\n",
      "  ✓ user_043: included (weight=46)\n",
      "  ✓ user_044: included (weight=46)\n",
      "  ✓ user_045: included (weight=46)\n",
      "  ✓ user_046: included (weight=46)\n",
      "  ✓ user_047: included (weight=46)\n",
      "  ✓ user_048: included (weight=46)\n",
      "  ✓ user_049: included (weight=46)\n",
      "  ✓ user_050: included (weight=46)\n",
      "  ✓ user_051: included (weight=46)\n",
      "  ✓ user_052: included (weight=46)\n",
      "  ✓ user_053: included (weight=46)\n",
      "  ✓ user_054: included (weight=46)\n",
      "  ✓ user_055: included (weight=46)\n",
      "  ✓ user_056: included (weight=46)\n",
      "  ✓ user_057: included (weight=46)\n",
      "  ✓ user_058: included (weight=46)\n",
      "  ✓ user_059: included (weight=46)\n",
      "  ✓ user_060: included (weight=46)\n",
      "  ✓ user_061: included (weight=46)\n",
      "  ✓ user_062: included (weight=46)\n",
      "  ✓ user_063: included (weight=46)\n",
      "  ✓ user_064: included (weight=46)\n",
      "  ✓ user_065: included (weight=46)\n",
      "  ✓ user_066: included (weight=46)\n",
      "  ✓ user_067: included (weight=46)\n",
      "  ✓ user_068: included (weight=46)\n",
      "  ✓ user_069: included (weight=46)\n",
      "  ✓ user_070: included (weight=46)\n",
      "  ✓ user_071: included (weight=46)\n",
      "  ✓ user_072: included (weight=46)\n",
      "  ✓ user_073: included (weight=46)\n",
      "  ✓ user_074: included (weight=46)\n",
      "  ✓ user_075: included (weight=46)\n",
      "  ✓ user_076: included (weight=46)\n",
      "  ✓ user_077: included (weight=46)\n",
      "  ✓ user_078: included (weight=46)\n",
      "  ✓ user_079: included (weight=46)\n",
      "  ✓ user_080: included (weight=46)\n",
      "  ✓ user_081: included (weight=46)\n",
      "  ✓ user_082: included (weight=46)\n",
      "  ✓ user_083: included (weight=46)\n",
      "  ✓ user_084: included (weight=46)\n",
      "  ✓ user_085: included (weight=46)\n",
      "  ✓ user_086: included (weight=46)\n",
      "  ✓ user_087: included (weight=46)\n",
      "  ✓ user_088: included (weight=46)\n",
      "  ✓ user_089: included (weight=46)\n",
      "  ✓ user_090: included (weight=46)\n",
      "  ✓ user_091: included (weight=46)\n",
      "  ✓ user_092: included (weight=46)\n",
      "  ✓ user_093: included (weight=46)\n",
      "  ✓ user_094: included (weight=46)\n",
      "  ✓ user_095: included (weight=46)\n",
      "  ✓ user_096: included (weight=46)\n",
      "  ✓ user_097: included (weight=46)\n",
      "  ✓ user_098: included (weight=46)\n",
      "  ✓ user_099: included (weight=46)\n",
      "  ✓ user_100: included (weight=46)\n",
      "  ✓ user_101: included (weight=46)\n",
      "  ✓ user_102: included (weight=46)\n",
      "  ✓ user_103: included (weight=46)\n",
      "  ✓ user_104: included (weight=46)\n",
      "  ✓ user_105: included (weight=46)\n",
      "  ✓ user_106: included (weight=46)\n",
      "  ✓ user_107: included (weight=46)\n",
      "  ✓ user_108: included (weight=46)\n",
      "  ✓ user_109: included (weight=46)\n",
      "  ✓ user_110: included (weight=46)\n",
      "  ✓ user_111: included (weight=46)\n",
      "  ✓ user_112: included (weight=46)\n",
      "  ✓ user_113: included (weight=46)\n",
      "  ✓ user_114: included (weight=46)\n",
      "  ✓ user_115: included (weight=46)\n",
      "  ✓ user_116: included (weight=46)\n",
      "  ✓ user_117: included (weight=46)\n",
      "  ✓ user_118: included (weight=46)\n",
      "  ✓ user_119: included (weight=46)\n",
      "  ✓ user_120: included (weight=46)\n",
      "  ✓ user_121: included (weight=46)\n",
      "  ✓ user_122: included (weight=46)\n",
      "  ✓ user_123: included (weight=46)\n",
      "  ✓ user_124: included (weight=46)\n",
      "  ✓ user_125: included (weight=46)\n",
      "  ✓ user_126: included (weight=46)\n",
      "  ✓ user_127: included (weight=46)\n",
      "  ✓ user_128: included (weight=46)\n",
      "\n",
      "✅ Aggregated 128 client(s) (total weight=5888) into:\n",
      "   Model  : /Users/sohinikar/FL/M.Tech_Dissertation/Server/global_model/global_lstm_model_AGG_20250830_015056.keras\n",
      "   Weights: /Users/sohinikar/FL/M.Tech_Dissertation/Server/global_model/global_lstm_model_AGG_20250830_015056.weights.h5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_clients(root: pathlib.Path) -> List[pathlib.Path]:\n",
    "    \"\"\"Return sorted user_* dirs that contain at least one *.weights.h5.\"\"\"\n",
    "    if not root.exists():\n",
    "        return []\n",
    "    clients = []\n",
    "    for udir in sorted(root.glob(\"user_*\")):\n",
    "        if list(udir.glob(\"*.weights.h5\")):\n",
    "            clients.append(udir)\n",
    "    return clients\n",
    "\n",
    "def load_client_sample_count(udir: pathlib.Path) -> int:\n",
    "    \"\"\"Read samples from params.json if present; else default to 1.\"\"\"\n",
    "    p = udir / \"params.json\"\n",
    "    if p.exists():\n",
    "        try:\n",
    "            with open(p, \"r\") as f:\n",
    "                j = json.load(f)\n",
    "            val = int(j.get(\"samples\", 1))\n",
    "            return max(val, 1)\n",
    "        except Exception:\n",
    "            return 1\n",
    "    return 1\n",
    "\n",
    "def pick_weights_file(udir: pathlib.Path) -> Optional[pathlib.Path]:\n",
    "    \"\"\"Pick the first *.weights.h5 in a user folder.\"\"\"\n",
    "    files = sorted(udir.glob(\"*.weights.h5\"))\n",
    "    return files[0] if files else None\n",
    "\n",
    "def fedavg_aggregate(global_model_path: pathlib.Path,\n",
    "                     clients_root: pathlib.Path,\n",
    "                     out_model_path: pathlib.Path,\n",
    "                     out_weights_path: pathlib.Path) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Perform FedAvg on all client weights under clients_root to update the global model.\n",
    "    Returns: (num_clients_used, total_weight)\n",
    "    \"\"\"\n",
    "    if not global_model_path.exists():\n",
    "        raise FileNotFoundError(f\"Global model not found: {global_model_path}\")\n",
    "\n",
    "    # Load a base model (for shapes)\n",
    "    base_model = tf.keras.models.load_model(str(global_model_path))\n",
    "    base_weights = base_model.get_weights()\n",
    "    if not base_weights:\n",
    "        raise RuntimeError(\"Global model has no weights (did you save an unbuilt model?).\")\n",
    "\n",
    "    # Accumulators\n",
    "    acc = [np.zeros_like(w, dtype=np.float64) for w in base_weights]\n",
    "    total_w = 0\n",
    "\n",
    "    clients = find_clients(clients_root)\n",
    "    print(f\"Found {len(clients)} client(s) in {clients_root}\")\n",
    "\n",
    "    used = 0\n",
    "    for udir in clients:\n",
    "        wfile = pick_weights_file(udir)\n",
    "        if wfile is None:\n",
    "            print(f\"  - {udir.name}: no *.weights.h5, skipping\")\n",
    "            continue\n",
    "\n",
    "        weight = load_client_sample_count(udir) if WEIGHT_BY_SAMPLES else 1\n",
    "\n",
    "        # Load a fresh model instance each time to avoid mutation issues\n",
    "        try:\n",
    "            m = tf.keras.models.load_model(str(global_model_path))\n",
    "            m.load_weights(str(wfile))\n",
    "            c_weights = m.get_weights()\n",
    "        except Exception as e:\n",
    "            print(f\"  - {udir.name}: failed to load weights ({wfile.name}): {e}, skipping\")\n",
    "            continue\n",
    "\n",
    "        # Shape check\n",
    "        if len(c_weights) != len(acc) or any(cw.shape != bw.shape for cw, bw in zip(c_weights, base_weights)):\n",
    "            print(f\"  - {udir.name}: weight shapes mismatch, skipping\")\n",
    "            continue\n",
    "\n",
    "        # Accumulate\n",
    "        for i in range(len(acc)):\n",
    "            acc[i] += weight * c_weights[i].astype(np.float64)\n",
    "        total_w += weight\n",
    "        used += 1\n",
    "        print(f\"  ✓ {udir.name}: included (weight={weight})\")\n",
    "\n",
    "    if used == 0 or total_w == 0:\n",
    "        raise RuntimeError(f\"No compatible client weights found in {clients_root}\")\n",
    "\n",
    "    # Compute weighted average and update base_model\n",
    "    avg = [(acc[i] / float(total_w)).astype(base_weights[i].dtype) for i in range(len(acc))]\n",
    "    base_model.set_weights(avg)\n",
    "\n",
    "    # Save updated global model + weights\n",
    "    out_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    base_model.save(str(out_model_path))\n",
    "    base_model.save_weights(str(out_weights_path))  # must end with \".weights.h5\"\n",
    "\n",
    "    print(f\"\\n✅ Aggregated {used} client(s) (total weight={total_w}) into:\")\n",
    "    print(f\"   Model  : {out_model_path}\")\n",
    "    print(f\"   Weights: {out_weights_path}\\n\")\n",
    "\n",
    "    return used, total_w\n",
    "\n",
    "def main():\n",
    "    # Aggregate FNN\n",
    "    try:\n",
    "        print(\"=== Aggregating FNN global model ===\")\n",
    "        fedavg_aggregate(FNN_GLOBAL_PATH, FNN_CLIENT_ROOT, FNN_OUT_MODEL, FNN_OUT_WEIGHTS)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ FNN aggregation failed: {e}\")\n",
    "\n",
    "    # Aggregate LSTM\n",
    "    try:\n",
    "        print(\"=== Aggregating LSTM global model ===\")\n",
    "        fedavg_aggregate(LSTM_GLOBAL_PATH, LSTM_CLIENT_ROOT, LSTM_OUT_MODEL, LSTM_OUT_WEIGHTS)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ LSTM aggregation failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b4a032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch strategy: TARBALL (token=no)\n",
      "=== FNN_MC: fetching client params ===\n",
      "Aggregating 128 FNN_MC client(s)…\n",
      "✅ Aggregated 128 client(s), total weight=5888\n",
      "   Model  : /Users/sohinikar/FL/M.Tech_Dissertation/Server/global_model/global_FNN_MC_model_AGG_20250830_125948.keras\n",
      "   Weights: /Users/sohinikar/FL/M.Tech_Dissertation/Server/global_model/global_FNN_MC_model_AGG_20250830_125948.weights.h5\n",
      "\n",
      "=== LSTM_MC: fetching client params ===\n",
      "Aggregating 30 LSTM_MC client(s)…\n",
      "✅ Aggregated 30 client(s), total weight=1380\n",
      "   Model  : /Users/sohinikar/FL/M.Tech_Dissertation/Server/global_model/global_lstm_MC_model_AGG_20250830_125948.keras\n",
      "   Weights: /Users/sohinikar/FL/M.Tech_Dissertation/Server/global_model/global_lstm_MC_model_AGG_20250830_125948.weights.h5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "FedAvg aggregator (Multiclass) that fetches client params from GitHub with\n",
    "rate-limit-safe strategies:\n",
    "  - If GITHUB_TOKEN present -> Contents API (high limit)\n",
    "  - Else -> single tarball download via codeload.github.com (no API limit)\n",
    "\n",
    "Aggregates (sample-weighted) into local global models:\n",
    "  - /Users/sohinikar/FL/M.Tech_Dissertation/Server/global_model/global_FNN_MC_model.keras\n",
    "  - /Users/sohinikar/FL/M.Tech_Dissertation/Server/global_model/global_lstm_MC_model.keras\n",
    "\n",
    "Client params on GitHub:\n",
    "  - M.Tech_Dissertation/Client/client_params/FNN_MC/user_*/user_*.weights.h5 + params.json\n",
    "  - M.Tech_Dissertation/Client/client_params/LSTM_MC/... (optional)\n",
    "\n",
    "Outputs (timestamped) written next to originals:\n",
    "  - *_AGG_<ts>.keras and *_AGG_<ts>.weights.h5\n",
    "\n",
    "Deps: pip install requests tensorflow\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import tarfile\n",
    "import pathlib\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "# ----------- Repo / Paths -----------\n",
    "OWNER = \"Triss11\"\n",
    "REPO  = \"FL\"\n",
    "REF   = \"main\"\n",
    "\n",
    "REMOTE_FNN_DIR  = \"M.Tech_Dissertation/Client/client_params/FNN_MC\"\n",
    "REMOTE_LSTM_DIR = \"M.Tech_Dissertation/Client/client_params/LSTM_MC\"  # optional\n",
    "\n",
    "FNN_GLOBAL_PATH  = pathlib.Path(\"/Users/sohinikar/FL/M.Tech_Dissertation/Server/global_model/global_FNN_MC_model.keras\")\n",
    "LSTM_GLOBAL_PATH = pathlib.Path(\"/Users/sohinikar/FL/M.Tech_Dissertation/Server/global_model/global_lstm_MC_model.keras\")\n",
    "\n",
    "TS = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "FNN_OUT_MODEL    = FNN_GLOBAL_PATH.with_name(f\"{FNN_GLOBAL_PATH.stem}_AGG_{TS}.keras\")\n",
    "FNN_OUT_WEIGHTS  = FNN_GLOBAL_PATH.with_name(f\"{FNN_GLOBAL_PATH.stem}_AGG_{TS}.weights.h5\")\n",
    "LSTM_OUT_MODEL   = LSTM_GLOBAL_PATH.with_name(f\"{LSTM_GLOBAL_PATH.stem}_AGG_{TS}.keras\")\n",
    "LSTM_OUT_WEIGHTS = LSTM_GLOBAL_PATH.with_name(f\"{LSTM_GLOBAL_PATH.stem}_AGG_{TS}.weights.h5\")\n",
    "\n",
    "CACHE_DIR = pathlib.Path(\"./_client_cache_mc\")\n",
    "WEIGHT_BY_SAMPLES = True\n",
    "\n",
    "# ----------- Strategy -----------\n",
    "# \"auto\": use API if token available, else tarball\n",
    "# \"api\" : force API listing/downloading\n",
    "# \"tarball\": force tarball extraction\n",
    "FETCH_STRATEGY = \"auto\"\n",
    "\n",
    "\n",
    "# ============ Helpers: HTTP / GitHub ============\n",
    "def gh_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    tok = os.environ.get(\"GITHUB_TOKEN\")\n",
    "    if tok:\n",
    "        s.headers.update({\"Authorization\": f\"Bearer {tok}\"})\n",
    "    s.headers.update({\"Accept\": \"application/vnd.github+json\"})\n",
    "    return s\n",
    "\n",
    "def gh_list_dir(session: requests.Session, path: str) -> List[dict]:\n",
    "    url = f\"https://api.github.com/repos/{OWNER}/{REPO}/contents/{path}?ref={REF}\"\n",
    "    r = session.get(url)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"GitHub list failed for {path}: {r.status_code} {r.text}\")\n",
    "    data = r.json()\n",
    "    if not isinstance(data, list):\n",
    "        raise RuntimeError(f\"{path} is not a directory on GitHub.\")\n",
    "    return data\n",
    "\n",
    "def gh_get_file(session: requests.Session, path: str) -> dict:\n",
    "    url = f\"https://api.github.com/repos/{OWNER}/{REPO}/contents/{path}?ref={REF}\"\n",
    "    r = session.get(url)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"GitHub get failed for {path}: {r.status_code} {r.text}\")\n",
    "    data = r.json()\n",
    "    if data.get(\"type\") != \"file\" or not data.get(\"download_url\"):\n",
    "        raise RuntimeError(f\"{path} is not a downloadable file.\")\n",
    "    return data\n",
    "\n",
    "def download_url_to(session: requests.Session, url: str, dest: pathlib.Path) -> pathlib.Path:\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with session.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dest, \"wb\") as f:\n",
    "            for chunk in r.iter_content(1024 * 1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    return dest\n",
    "\n",
    "\n",
    "# ============ Strategy A: API (tokened) ============\n",
    "def fetch_clients_via_api(session: requests.Session, remote_root: str, cache_root: pathlib.Path) -> List[Tuple[pathlib.Path, int]]:\n",
    "    \"\"\"\n",
    "    Downloads user_*/user_*.weights.h5 and params.json using the Contents API.\n",
    "    Assumes weights are named exactly 'user_xxx.weights.h5'.\n",
    "    \"\"\"\n",
    "    users = gh_list_dir(session, remote_root)\n",
    "    results = []\n",
    "    for ent in sorted(users, key=lambda e: e.get(\"name\", \"\")):\n",
    "        if ent.get(\"type\") != \"dir\":\n",
    "            continue\n",
    "        user = ent[\"name\"]  # e.g., user_001\n",
    "        try:\n",
    "            weight_name = f\"{user}.weights.h5\"\n",
    "            weight_path = f\"{remote_root}/{user}/{weight_name}\"\n",
    "            params_path = f\"{remote_root}/{user}/params.json\"\n",
    "\n",
    "            # Download weights (required)\n",
    "            w_json = gh_get_file(session, weight_path)\n",
    "            w_local = cache_root / user / weight_name\n",
    "            download_url_to(session, w_json[\"download_url\"], w_local)\n",
    "\n",
    "            # Download params.json (optional)\n",
    "            samples = 1\n",
    "            try:\n",
    "                p_json = gh_get_file(session, params_path)\n",
    "                p_local = cache_root / user / \"params.json\"\n",
    "                download_url_to(session, p_json[\"download_url\"], p_local)\n",
    "                with open(p_local, \"r\") as f:\n",
    "                    j = json.load(f)\n",
    "                samples = max(int(j.get(\"samples\", 1)), 1)\n",
    "            except Exception:\n",
    "                samples = 1\n",
    "\n",
    "            results.append((w_local, samples))\n",
    "            print(f\"  ✓ {user}: downloaded {weight_name}, samples={samples}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - {user}: {e} (skipping)\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============ Strategy B: Tarball (no API limit) ============\n",
    "def download_repo_tarball(session: requests.Session, cache_dir: pathlib.Path) -> pathlib.Path:\n",
    "    \"\"\"\n",
    "    Downloads the repo tar.gz via codeload (not API rate-limited).\n",
    "    \"\"\"\n",
    "    tar_url = f\"https://codeload.github.com/{OWNER}/{REPO}/tar.gz/{REF}\"\n",
    "    tar_path = cache_dir / f\"{REPO}_{REF}.tar.gz\"\n",
    "    if tar_path.exists() and tar_path.stat().st_size > 0:\n",
    "        return tar_path\n",
    "    tar_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with session.get(tar_url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(tar_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(1024 * 1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    return tar_path\n",
    "\n",
    "def extract_clients_from_tarball(tar_path: pathlib.Path, remote_root: str, cache_root: pathlib.Path) -> List[Tuple[pathlib.Path, int]]:\n",
    "    \"\"\"\n",
    "    Extract only files under `remote_root` (weights + params.json) to cache_root.\n",
    "    Returns [(local_weights_path, samples_weight)].\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    cache_root.mkdir(parents=True, exist_ok=True)\n",
    "    with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "        # tar member names look like: \"<owner>-<repo>-<sha>/{repo_tree...}\"\n",
    "        for m in tar.getmembers():\n",
    "            if not m.isfile():\n",
    "                continue\n",
    "            # match the remote_root path inside the tar\n",
    "            # ensure \"/<remote_root>/\" substring to avoid partial matches\n",
    "            needle = f\"/{remote_root}/\"\n",
    "            if needle not in m.name:\n",
    "                continue\n",
    "            rel = m.name.split(needle, 1)[1]  # e.g., \"user_001/user_001.weights.h5\"\n",
    "            # Save only weights and params.json\n",
    "            if not (rel.endswith(\".weights.h5\") or rel.endswith(\"params.json\")):\n",
    "                continue\n",
    "            dest = cache_root / rel\n",
    "            dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with tar.extractfile(m) as src, open(dest, \"wb\") as out:\n",
    "                out.write(src.read())\n",
    "\n",
    "    # Build list and read samples\n",
    "    # We expect weights at user_xxx/user_xxx.weights.h5 (from earlier training script)\n",
    "    for user_dir in sorted((cache_root).glob(\"user_*\")):\n",
    "        if not user_dir.is_dir():\n",
    "            continue\n",
    "        weight_file = next(user_dir.glob(\"*.weights.h5\"), None)\n",
    "        if not weight_file:\n",
    "            continue\n",
    "        samples = 1\n",
    "        p = user_dir / \"params.json\"\n",
    "        if p.exists():\n",
    "            try:\n",
    "                with open(p, \"r\") as f:\n",
    "                    j = json.load(f)\n",
    "                samples = max(int(j.get(\"samples\", 1)), 1)\n",
    "            except Exception:\n",
    "                samples = 1\n",
    "        results.append((weight_file, samples))\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============ FedAvg aggregation ============\n",
    "def fedavg_aggregate(global_model_path: pathlib.Path,\n",
    "                     client_weights: List[Tuple[pathlib.Path, int]],\n",
    "                     out_model_path: pathlib.Path,\n",
    "                     out_weights_path: pathlib.Path):\n",
    "    if not global_model_path.exists():\n",
    "        raise FileNotFoundError(f\"Global model not found: {global_model_path}\")\n",
    "    model = tf.keras.models.load_model(str(global_model_path))\n",
    "    base_w = model.get_weights()\n",
    "    if not base_w:\n",
    "        raise RuntimeError(\"Global model has no weights (unbuilt?).\")\n",
    "\n",
    "    acc = [np.zeros_like(w, dtype=np.float64) for w in base_w]\n",
    "    total = 0\n",
    "    used = 0\n",
    "\n",
    "    for w_path, weight in client_weights:\n",
    "        try:\n",
    "            m = tf.keras.models.load_model(str(global_model_path))\n",
    "            m.load_weights(str(w_path))\n",
    "            cw = m.get_weights()\n",
    "        except Exception as e:\n",
    "            print(f\"  - {w_path.parent.name}: failed to load weights: {e} (skip)\")\n",
    "            continue\n",
    "\n",
    "        if len(cw) != len(base_w) or any(c.shape != b.shape for c, b in zip(cw, base_w)):\n",
    "            print(f\"  - {w_path.parent.name}: weight shape mismatch (skip)\")\n",
    "            continue\n",
    "\n",
    "        for i in range(len(acc)):\n",
    "            acc[i] += weight * cw[i].astype(np.float64)\n",
    "        total += weight\n",
    "        used += 1\n",
    "\n",
    "    if used == 0 or total == 0:\n",
    "        raise RuntimeError(\"No compatible client weights to aggregate.\")\n",
    "\n",
    "    avg = [(acc[i] / float(total)).astype(base_w[i].dtype) for i in range(len(acc))]\n",
    "    model.set_weights(avg)\n",
    "\n",
    "    out_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    model.save(str(out_model_path))\n",
    "    model.save_weights(str(out_weights_path))  # Keras 3 requires .weights.h5\n",
    "    print(f\"✅ Aggregated {used} client(s), total weight={total}\")\n",
    "    print(f\"   Model  : {out_model_path}\")\n",
    "    print(f\"   Weights: {out_weights_path}\\n\")\n",
    "\n",
    "\n",
    "# ============ Main ============\n",
    "def main():\n",
    "    CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    session = gh_session()\n",
    "    token_present = os.environ.get(\"GITHUB_TOKEN\") is not None\n",
    "\n",
    "    # Pick strategy\n",
    "    strategy = FETCH_STRATEGY\n",
    "    if strategy == \"auto\":\n",
    "        strategy = \"api\" if token_present else \"tarball\"\n",
    "\n",
    "    print(f\"Fetch strategy: {strategy.upper()} (token={'yes' if token_present else 'no'})\")\n",
    "\n",
    "    # -------- FNN_MC --------\n",
    "    print(\"=== FNN_MC: fetching client params ===\")\n",
    "    if strategy == \"api\":\n",
    "        fnn_clients = fetch_clients_via_api(session, REMOTE_FNN_DIR, CACHE_DIR / \"FNN_MC\")\n",
    "    else:\n",
    "        tar_path = download_repo_tarball(session, CACHE_DIR)\n",
    "        fnn_clients = extract_clients_from_tarball(tar_path, REMOTE_FNN_DIR, CACHE_DIR / \"FNN_MC\")\n",
    "\n",
    "    if fnn_clients:\n",
    "        print(f\"Aggregating {len(fnn_clients)} FNN_MC client(s)…\")\n",
    "        fedavg_aggregate(FNN_GLOBAL_PATH, fnn_clients, FNN_OUT_MODEL, FNN_OUT_WEIGHTS)\n",
    "    else:\n",
    "        print(\"⚠️  No FNN_MC clients found to aggregate.\")\n",
    "\n",
    "    # -------- LSTM_MC (optional) --------\n",
    "    print(\"=== LSTM_MC: fetching client params ===\")\n",
    "    try:\n",
    "        if strategy == \"api\":\n",
    "            lstm_clients = fetch_clients_via_api(session, REMOTE_LSTM_DIR, CACHE_DIR / \"LSTM_MC\")\n",
    "        else:\n",
    "            tar_path = download_repo_tarball(session, CACHE_DIR)\n",
    "            lstm_clients = extract_clients_from_tarball(tar_path, REMOTE_LSTM_DIR, CACHE_DIR / \"LSTM_MC\")\n",
    "    except Exception as e:\n",
    "        print(f\"ℹ️  Skipping LSTM_MC fetch: {e}\")\n",
    "        lstm_clients = []\n",
    "\n",
    "    if lstm_clients:\n",
    "        print(f\"Aggregating {len(lstm_clients)} LSTM_MC client(s)…\")\n",
    "        fedavg_aggregate(LSTM_GLOBAL_PATH, lstm_clients, LSTM_OUT_MODEL, LSTM_OUT_WEIGHTS)\n",
    "    else:\n",
    "        print(\"ℹ️  No LSTM_MC clients found (or path missing). Skipping.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
